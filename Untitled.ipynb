{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 -5  0 -5 -1  1  0  0]\n",
      " [10  5 10  5  0  0 -1  1]]\n",
      "[ 0 -5  0 -5 -1  1  0  0]\n",
      "[[ 0 -5 -1]\n",
      " [10  5  1]]\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "bound =  [0, 10, -5, 5]\n",
    "abound=[0, 10, -5, 5, 0, 10, -5, 5]\n",
    "_st_range = np.array([bound[:2], bound[2:], [-1, 1]]).T\n",
    "strange = np.array([abound[:2],abound[2:4],abound[4:6],abound[6:],[-1,0],[1, 0],[0, -1],[0, 1]]).T\n",
    "print(strange)\n",
    "print(strange[0])\n",
    "print(_st_range)\n",
    "print(_st_range[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLagent(Marble):\n",
    "    def __init__(self,env,r=0.01,nInputs = 3,nOutputs = 1,nSamples = 1,nHiddens = 5):\n",
    "        self.env=env\n",
    "        self.rhoh = self.rhoo = r\n",
    "        self.rh = self.rhoh / (nSamples*nOutputs)\n",
    "        self.ro = self.rhoo / (nSamples*nOutputs)\n",
    "        self.V = 0.1*2*(np.random.uniform(size=(nInputs+1,nHiddens))-0.5)\n",
    "        self.W = 0.1*2*(np.random.uniform(size=(1+nHiddens,nOutputs))-0.5)\n",
    "        self.fig = plt.figure(figsize=(8, 8))\n",
    "        self.stdX = Standardizer(self.env.get_state_range())\n",
    "    def addOnes(self,A):\n",
    "        return np.insert(A, 0, 1, axis=len(np.array(A).shape)-1)\n",
    "\n",
    "    def forward(self,X):\n",
    "        #X = self.stdX.standardize(X)\n",
    "        # Forward pass on training data\n",
    "        X1 = self.addOnes(X)\n",
    "        Z = np.tanh(X1 @ self.V)\n",
    "        Z1 = self.addOnes(Z)\n",
    "        Y = Z1 @ self.W\n",
    "        return Y, Z\n",
    "\n",
    "    def as_array(self,A):\n",
    "        A = np.array(A)\n",
    "        if len(A.shape) == 1:\n",
    "            return A.reshape((1, -1))\n",
    "        return A\n",
    "\n",
    "    def backward(self,error, Z, X):\n",
    "    \n",
    "        ### make sure the array shapes\n",
    "        X = self.as_array(X)\n",
    "        Z = self.as_array(Z)\n",
    "        E = self.as_array(error)\n",
    "    \n",
    "        Z1 = self.addOnes(Z)\n",
    "        X1 = self.addOnes(X)\n",
    "\n",
    "        # Backward pass - the backpropagation and weight update steps\n",
    "        dV = self.rh * X1.T @ ( ( E @ self.W[1:,:].T) * (1-Z**2))\n",
    "        dW = self.ro * Z1.T @ E\n",
    "        return dV, dW\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self,e, s,n_actions):\n",
    "        if np.random.rand() < e:\n",
    "            return np.random.randint(n_actions) - 1\n",
    "        else:\n",
    "            Q, _ = self.forward(np.hstack((np.tile(s, (3,1)), env.get_actions().reshape((-1, 1)))))\n",
    "            max_as = np.where(Q == np.max(Q))[0] - 1 # index to action value\n",
    "            return np.random.choice(max_as)\n",
    "    def use(self,k=300,n=3,steps=500,g=.9,verb=False,epsilon=1,final=0.1):\n",
    "        K = k \n",
    "        n_actions = n \n",
    "        max_steps = steps\n",
    "        gamma = g\n",
    "        verbose = verb\n",
    "        epsilon = epsilon\n",
    "        final_epsilon = final\n",
    "        epsilon_decay =  np.exp(np.log(final_epsilon) / K)\n",
    "        rtrace = []\n",
    "        etrace = [epsilon]\n",
    "        for j in range(K):\n",
    "\n",
    "            if verbose: print(\"\\tepisode #\", j, \"   \",end=\"\")\n",
    "            env.init([3,0]) #[float(np.random.randint(-5, 5, 1)), 0])\n",
    "            s = env.get_cur_state()\n",
    "            # selection an action\n",
    "            a = self.epsilon_greedy(epsilon, s,n_actions)\n",
    "\n",
    "            rewards = []\n",
    "            trace = np.array(s)\n",
    "            for step in range(max_steps):\n",
    "                if verbose: print(\"\\tstep #\", step, \"   \",end=\"\")\n",
    "                # move\n",
    "                r1 = env.next(a)\n",
    "                s1 = env.get_cur_state()\n",
    "                a1 = self.epsilon_greedy(epsilon, s1,n_actions)\n",
    "\n",
    "                rewards.append(r1)\n",
    "                trace = np.vstack((trace, s1))\n",
    "                # update neural networks\n",
    "                Q1, _ = self.forward(np.hstack((s1, a1)))  # output of neural network is Q for next state\n",
    "                Q, Z = self.forward(np.hstack((s, a)))  # output of neural network is Q for next state\n",
    "                error = r1 + gamma * Q1 - Q  # use action value as index by adding one\n",
    "                dV, dW = self.backward(error, Z, np.hstack((s, a)))\n",
    "                self.V += dV\n",
    "                self.W += dW\n",
    "\n",
    "                s = s1\n",
    "                a = a1\n",
    "        \n",
    "            epsilon *= epsilon_decay\n",
    "            etrace.append(epsilon)\n",
    "\n",
    "\n",
    "            if verbose: print(\"Done (\", np.sum(rewards), \")\", step)\n",
    "\n",
    "            rtrace.append(np.sum(rewards))\n",
    "\n",
    "            last_plot = (j == K-1)\n",
    "    \n",
    "            if j % 10 == 0 or last_plot:\n",
    "                plt.clf()\n",
    "                self.fig.add_subplot(221)\n",
    "                plt.plot(rtrace, \"b-\")\n",
    "                plt.ylabel(\"sum of rewards\")\n",
    "\n",
    "                self.fig.add_subplot(222)\n",
    "                plt.plot(etrace, \"-\")\n",
    "                plt.ylabel(\"p(random action), $\\epsilon$\")\n",
    "\n",
    "                # contour plot for Q\n",
    "                self.fig.add_subplot(223)\n",
    "                xs, ys = np.meshgrid(np.linspace(0, 10, 100), np.linspace(-5, 5, 100))\n",
    "                X = np.vstack((xs.flat, ys.flat)).T\n",
    "\n",
    "                Q = np.array([self.forward(np.hstack((x, a)))[0] for a in [-1,0,1] for x in X])\n",
    "                maxQ = np.max(Q.reshape((3, -1)), axis=0)\n",
    "                cs = plt.contourf(xs, ys, maxQ.reshape(xs.shape))\n",
    "                plt.colorbar(cs)\n",
    "                plt.text(env.Goal, 0, 'G')\n",
    "                plt.ylabel(\"max Q\")\n",
    "\n",
    "                # plot traces\n",
    "                self.fig.add_subplot(224)\n",
    "                #print(trace)\n",
    "                plt.plot(trace[:, 0], trace[:, 1], \"k-\")\n",
    "                plt.fill_between([env.Goal-1, env.Goal+1], [-5, -5],[5, 5], color='red', alpha=0.3)\n",
    "                plt.title(\"trace of last episode\")\n",
    "                plt.xlim([0, 10])\n",
    "                plt.ylim([-5, 5])\n",
    "\n",
    "                plt.suptitle(''.join([\"Episode \",str(j)]))\n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "                plt.draw()\n",
    "\n",
    "                ipd.clear_output(wait=True)\n",
    "                ipd.display(self.fig)\n",
    "        ipd.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Marble():\n",
    "    \"\"\" 1d marble problem\n",
    "        \n",
    "        states: x, dx\n",
    "        action: action [-1,1]\n",
    "\n",
    "\n",
    "        |            ___                     |\n",
    "        |___________|///|____G_______________|\n",
    "                    <- ->\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,goal=5, **params):\n",
    "        # len(state) + action (1)\n",
    "        self.n_state = 2\n",
    "        self.n_action = 1\n",
    "        self.Goal = goal\n",
    "        self.bound = params.pop('bound', [0, 10, -5, 5])\n",
    "        if len(self.bound) != 4:\n",
    "             self.bound = self.bound[:2] + [-5, 5]\n",
    "\n",
    "        self._st_range = np.array([self.bound[:2], self.bound[2:], [-1, 1]]).T\n",
    "        self.nnNI = self.n_state + 1\n",
    "        self.goal_width = 1\n",
    "        \n",
    "        self._s = [0, 0]\n",
    "\n",
    "    def init(self, start=None):\n",
    "        if start is not None:\n",
    "            self._s = start\n",
    "        else: \n",
    "            self._s = [np.random.randint(self.bound[0], self.bound[1]), 0.]\n",
    "        return self._s\n",
    "       \n",
    "    def get_random_action(self):\n",
    "        return float(np.random.randint(3) -1) # discrete action\n",
    "\n",
    "    def get_bound_act(self, a):\n",
    "        if a[0] > 1:\n",
    "            return 1\n",
    "        elif a[0] < -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return a[0]\n",
    "\n",
    "    def next(self, a) :\n",
    "        s = self._s\n",
    "        if isinstance(a, collections.Iterable):\n",
    "            a = a[0]\n",
    "        s1 = copy(s)\n",
    "        dT = 0.1\n",
    "        s1[0] += dT * s[1]  \n",
    "        s1[1] += dT * ( 2*a - 0.2 * s[1] )\n",
    "\n",
    "        # adjust velocity when outside of the track\n",
    "        if s1[0] < self.bound[0]:\n",
    "            s1[:]  = [self.bound[0], 0]\n",
    "        elif s1[0] > self.bound[1] :\n",
    "            s1[:] = [self.bound[1], 0]\n",
    "        # clipping the velocity\n",
    "        s1[1] = np.clip(s1[1], self._st_range[0, 1],\n",
    "                               self._st_range[1, 1])\n",
    "\n",
    "        self._s =  s1\n",
    "        return self.get_reward(s, s1, a)\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self._s\n",
    "\n",
    "    def get_reward(self,s,s1,a):\n",
    "        return 1 if abs(s1[0] - self.Goal) < self.goal_width else 0\n",
    "\n",
    "    def get_state_range(self):\n",
    "        return self._st_range\n",
    "\n",
    "    def get_actions(self):\n",
    "        return np.array([-1., 0., 1.])\n",
    "\n",
    "    def get_action_index(self, action):\n",
    "        return np.where(np.array([-1, 0, 1]) == action)[0][0]\n",
    "\n",
    "    def draw_trajectory(self, smplX):\n",
    "        if smplX.shape[1] == 1: return\n",
    "        plt.plot(smplX[:,0],smplX[:,1])\n",
    "        plt.axis([self.bound[0], self.bound[1],-5,5])\n",
    "        plt.plot(smplX[0,0],smplX[0,1],'go')\n",
    "        plt.plot(self.Goal,0,'ro')\n",
    "        # draw a goal region\n",
    "        plt.fill_between([self.Goal-self.goal_width, self.Goal+self.goal_width],\n",
    "                         [-5,-5], [5,5],\n",
    "                         color=\"red\", alpha=0.3)\n",
    "        plt.xlabel(\"s\") \n",
    "        plt.ylabel(\"s dot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
